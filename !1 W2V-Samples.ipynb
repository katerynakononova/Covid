{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import gensim, logging\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "світовий пандемія принести невизначеність вимушений реагувати надшвидко маючи право помилка бізнес винаходити модель благодійний адаптуватися кризовий умова пропонуючи підхід рішення проєкт травень директорка виступити віртуальний саміт благодійництво тема фандрайзинг специфіка ковіднога долучитися дискусіїй секція благодійник бізнес адаптивність криза запрошувати прийняти саміт участь безкоштовний реєстрація посилання\n"
     ]
    }
   ],
   "source": [
    "# Importing prepared dataset\n",
    "import pickle\n",
    "input = open('my_corpus-21-sql.pql', 'rb')\n",
    "obj = pickle.load(input)\n",
    "input.close()\n",
    "\n",
    "txt = obj['txt']\n",
    "corpus = obj['corpus']\n",
    "words_list = obj['words_list']\n",
    "all_words = obj['all_words']\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "w1, w2 = train_test_split(words_list, test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.KeyedVectors.load_word2vec_format('ubercorpus.lowercased.lemmatized.word2vec.300d.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_embed(words, corpus):\n",
    "    x_sent_embed = []\n",
    "    count_words, count_non_words = 0, 0  \n",
    "    \n",
    "    # recover the embedding of each sentence with the average of the vector that composes it\n",
    "    # sent - sentence\n",
    "    for sent in corpus:\n",
    "        # average embedding of all words in a sentence\n",
    "        sent_embed = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                # if word is present in the dictionary - add its vector representation\n",
    "                count_words += 1\n",
    "                sent_embed.append(model[word])\n",
    "            except KeyError:\n",
    "                # if word is not in the dictionary - add a zero vector\n",
    "                count_non_words += 1\n",
    "                sent_embed.append([0] * 300)\n",
    "        \n",
    "        # add a sentence vector to the list\n",
    "        x_sent_embed.append(np.mean(sent_embed, axis=0).tolist())\n",
    "                    \n",
    "    print(count_non_words, \"out of\", count_words, \"words were not found in the vocabulary.\")\n",
    "    \n",
    "    return x_sent_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kater\\anaconda3\\envs\\covid\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\kater\\anaconda3\\envs\\covid\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6488 out of 143479 words were not found in the vocabulary.\n",
      "6501 out of 159010 words were not found in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "x1 = txt_embed(words, w1)\n",
    "x2 = txt_embed(words, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "192\n",
      "355\n",
      "571\n",
      "846\n",
      "851\n",
      "1054\n",
      "1219\n"
     ]
    }
   ],
   "source": [
    "embed1 = []\n",
    "for i,v in enumerate(x1): \n",
    "    if np.any(np.isnan(v)) != True:\n",
    "        embed1.append(np.array(v))\n",
    "    else:\n",
    "        \n",
    "        embed1.append(np.array([0] * 300))\n",
    "        print(i)\n",
    "em1 = np.vstack(embed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "327\n"
     ]
    }
   ],
   "source": [
    "embed2 = []\n",
    "for i,v in enumerate(x2): \n",
    "    if np.any(np.isnan(v)) != True:\n",
    "        embed2.append(np.array(v))\n",
    "    else:\n",
    "        embed2.append(np.array([0] * 300))\n",
    "        print(i)\n",
    "em2 = np.vstack(embed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Clusters\n",
    "k = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "km1 = KMeans(n_clusters = k).fit(em1)\n",
    "km2 = KMeans(n_clusters = k).fit(em2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "(adjusted_rand_score(km1.labels_.tolist(),km2.labels_.tolist()).round(3))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc1 = AgglomerativeClustering(n_clusters = k, affinity = 'euclidean', linkage = 'ward').fit_predict(em1)\n",
    "hc2 = AgglomerativeClustering(n_clusters = k, affinity = 'euclidean', linkage = 'ward').fit_predict(em2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "(adjusted_rand_score(hc1,hc2).round(3))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "mshift1 = MeanShift(bandwidth=20).fit(em1)\n",
    "mshift2 = MeanShift(bandwidth=20).fit(em2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "(adjusted_rand_score(mshift1.labels_,mshift2.labels_).round(3))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(mshift1.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdbscan\n",
    "hdb1 = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=7).fit(em1)\n",
    "hdb2 = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=7).fit(em2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "(adjusted_rand_score(hdb1.labels_,hdb2.labels_).round(3))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(hdb2.labels_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
