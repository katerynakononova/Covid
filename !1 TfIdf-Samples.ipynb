{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfIdf-Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install\n",
    "#!pip install -r requirements.txt\n",
    "#pip install numpy==1.20.0\n",
    "#pip install pandas\n",
    "#pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "світовий пандемія принести невизначеність вимушений реагувати надшвидко маючи право помилка бізнес винаходити модель благодійний адаптуватися кризовий умова пропонуючи підхід рішення проєкт травень директорка виступити віртуальний саміт благодійництво тема фандрайзинг специфіка ковіднога долучитися дискусіїй секція благодійник бізнес адаптивність криза запрошувати прийняти саміт участь безкоштовний реєстрація посилання\n"
     ]
    }
   ],
   "source": [
    "# Importing prepared dataset\n",
    "import pickle\n",
    "input = open('my_corpus-21-sql.pql', 'rb')\n",
    "obj = pickle.load(input)\n",
    "input.close()\n",
    "\n",
    "txt = obj['txt']\n",
    "corpus = obj['corpus']\n",
    "all_words = obj['all_words']\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "c1, c2 = train_test_split(corpus, test_size=0.5, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=0.001, use_idf=True, ngram_range=(1,3))\n",
    "                            #max_df=0.8, max_features=10000\n",
    "tfidf_matrix1 = vectorizer.fit_transform(c1)\n",
    "tfidf_matrix2 = vectorizer.fit_transform(c2)\n",
    "#terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install clusteval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Clusters\n",
    "k = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km1 = KMeans(n_clusters = k).fit(tfidf_matrix1)\n",
    "km2 = KMeans(n_clusters = k).fit(tfidf_matrix2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "(adjusted_rand_score(km1.labels_.tolist(),km2.labels_.tolist()).round(3))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model = LatentDirichletAllocation(n_components = k,           # Number of topics\n",
    "                                      max_iter = 20,               # Max learning iterations\n",
    "                                      random_state = 100,          # Random state\n",
    "                                      batch_size = 128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output1 = lda_model.fit_transform(tfidf_matrix1)\n",
    "lda_output2 = lda_model.fit_transform(tfidf_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA-K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-means clustering\n",
    "kmLDA1 = KMeans(n_clusters = k).fit(lda_output1)\n",
    "kmLDA2 = KMeans(n_clusters = k).fit(lda_output2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "(adjusted_rand_score(kmLDA1.labels_.tolist(),kmLDA2.labels_.tolist()).round(3))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hc1 = AgglomerativeClustering(n_clusters = k, affinity = 'euclidean', linkage = 'ward').fit_predict(lda_output1)\n",
    "hc2 = AgglomerativeClustering(n_clusters = k, affinity = 'euclidean', linkage = 'ward').fit_predict(lda_output2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "(adjusted_rand_score(hc1,hc2).round(3))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdbscan\n",
    "hdb1 = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=12).fit(tfidf_matrix1)\n",
    "hdb2 = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=12).fit(tfidf_matrix2)\n",
    "\n",
    "# Comparing Clustering Algorithms\n",
    "(adjusted_rand_score(hdb1.labels_,hdb2.labels_).round(3))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(hdb2.labels_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
